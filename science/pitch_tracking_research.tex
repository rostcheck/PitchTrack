\documentclass[conference]{IEEEtran}
\IEEEoverridecommandlockouts
% The preceding line is only needed to identify funding in the first footnote. If that is unneeded, please comment it out.
\usepackage{cite}
\usepackage{amsmath,amssymb,amsfonts}
\usepackage{algorithmic}
\usepackage{graphicx}
\usepackage{textcomp}
\usepackage{xcolor}
\usepackage{listings}
\usepackage{hyperref}
\usepackage{url}

\lstset{
  basicstyle=\ttfamily\small,
  commentstyle=\color{gray},
  keywordstyle=\color{blue},
  stringstyle=\color{red},
  breaklines=true,
  showstringspaces=false
}

\def\BibTeX{{\rm B\kern-.05em{\sc i\kern-.025em b}\kern-.08em
    T\kern-.1667em\lower.7ex\hbox{E}\kern-.125emX}}
    
\begin{document}

\title{Development and Evaluation of Pitch Detection Algorithms for a Karaoke Training Application\\
}

\author{\IEEEauthorblockN{David Rostcheck}
\IEEEauthorblockA{\textit{Independent Researcher} \\
david@rostcheck.com}
\and
\IEEEauthorblockN{Amazon Q Developer CLI w/ Anthropic Claude Sonnet 3.7}
\IEEEauthorblockA{\textit{AI Assistant} \\
}
}

\maketitle

\begin{abstract}
This paper presents the research, development, and evaluation of pitch detection algorithms and vocal line extraction methods for a karaoke training application called PitchTrack. We compare several established pitch detection algorithms including YIN, pYIN (probabilistic YIN), CREPE (Convolutional Representation for Pitch Estimation), and McLeod Pitch Method (MPM), evaluating their accuracy, computational efficiency, and suitability for real-time vocal pitch tracking. We implement and test these algorithms using Python libraries including librosa and aubio, and develop post-processing techniques to improve pitch tracking for vocal applications. Additionally, we evaluate vocal line extraction methods including Spleeter and Demucs to isolate vocals from mixed audio recordings, enabling users to practice with just the vocal line. Our findings indicate that pYIN offers the best balance of accuracy and computational efficiency for vocal pitch tracking, while Demucs provides superior vocal isolation quality despite longer processing times. We also discuss the development of a visualization system that provides intuitive feedback to users through a piano roll interface.
\end{abstract}

\begin{IEEEkeywords}
pitch detection, audio source separation, vocal extraction, music information retrieval, karaoke, vocal training, pYIN, Demucs, real-time audio processing
\end{IEEEkeywords}

\section{Introduction}
Accurate pitch detection is a fundamental challenge in music information retrieval and audio signal processing. For applications like vocal training and karaoke systems, the ability to precisely track the fundamental frequency of a singing voice in real-time is essential for providing meaningful feedback to users. However, vocal pitch detection presents unique challenges due to the complex harmonic structure of the human voice, presence of vibrato, rapid pitch transitions, and varying timbres across different singers \cite{mauch2014pyin}.

In this paper, we describe the research and development process for PitchTrack, a karaoke training application that provides real-time visual feedback on vocal pitch. We focus on the evaluation of different pitch detection algorithms, their implementation using available libraries, and the development of post-processing techniques to improve pitch tracking specifically for vocal applications.

The main contributions of this paper are:
\begin{itemize}
    \item A comparative analysis of pitch detection algorithms for vocal applications
    \item Implementation and evaluation of post-processing techniques to improve pitch tracking quality
    \item Development of a real-time visualization system for intuitive pitch feedback
    \item Practical recommendations for implementing pitch detection in vocal training applications
\end{itemize}

\section{Background and Related Work}

\subsection{Pitch Detection Algorithms}
Pitch detection algorithms can be broadly categorized into time-domain and frequency-domain approaches \cite{gerhard2003pitch}. Time-domain methods typically analyze the periodicity of the waveform, while frequency-domain methods examine the spectral content of the signal.

\subsubsection{YIN Algorithm}
The YIN algorithm \cite{cheveigne2002yin} is a widely used time-domain pitch detection method based on autocorrelation with additional processing steps to reduce errors. It computes a modified autocorrelation function called the cumulative mean normalized difference function, which helps reduce octave errors common in basic autocorrelation methods.

The YIN algorithm can be summarized in the following steps:
\begin{enumerate}
    \item Compute the difference function:
    \begin{equation}
    d_t(\tau) = \sum_{j=1}^{W} (x_j - x_{j+\tau})^2
    \end{equation}
    where $x$ is the input signal, $\tau$ is the lag, and $W$ is the window size.
    
    \item Compute the cumulative mean normalized difference function:
    \begin{equation}
    d'_t(\tau) = 
    \begin{cases}
    1, & \text{if}\ \tau = 0 \\
    \frac{d_t(\tau)}{\frac{1}{\tau}\sum_{j=1}^{\tau}d_t(j)}, & \text{otherwise}
    \end{cases}
    \end{equation}
    
    \item Find the minimum of $d'_t(\tau)$ that is below a threshold.
    
    \item Refine the estimate using parabolic interpolation.
\end{enumerate}

\subsubsection{pYIN (Probabilistic YIN)}
pYIN \cite{mauch2014pyin} extends the YIN algorithm by incorporating a probabilistic model to improve pitch tracking accuracy. It uses multiple pitch candidates from the YIN algorithm and applies a hidden Markov model (HMM) to find the most likely pitch trajectory over time. This approach is particularly effective for vocal pitch tracking as it better handles note transitions and vibrato.

\subsubsection{CREPE (Convolutional Representation for Pitch Estimation)}
CREPE \cite{kim2018crepe} represents a more recent approach using deep learning. It employs a convolutional neural network trained on a large dataset of labeled audio to predict pitch. CREPE has demonstrated state-of-the-art accuracy, particularly for vocal pitch tracking, but comes with higher computational requirements.

\subsubsection{McLeod Pitch Method (MPM)}
The McLeod Pitch Method \cite{mcleod2005fast} is based on the normalized square difference function (NSDF) and is designed to be computationally efficient while maintaining good accuracy. It is particularly effective at handling noisy signals.

\subsection{Libraries for Pitch Detection}
Several libraries implement these algorithms and provide accessible interfaces for developers:

\begin{itemize}
    \item \textbf{Aubio}: A C library with Python bindings that implements YIN, YinFFT, and other algorithms.
    \item \textbf{Librosa}: A Python package for music and audio analysis that includes implementations of pYIN.
    \item \textbf{TarsosDSP}: A Java library that implements MPM and other algorithms.
    \item \textbf{CREPE}: A TensorFlow implementation of the CREPE algorithm.
\end{itemize}

\section{Methodology}

\subsection{Evaluation Criteria}
We evaluated pitch detection algorithms based on the following criteria:
\begin{itemize}
    \item \textbf{Accuracy}: How precisely the algorithm tracks pitch variations, particularly for vocal audio with vibrato and transitions.
    \item \textbf{Latency}: The delay between audio input and pitch detection output, critical for real-time applications.
    \item \textbf{Robustness}: How well the algorithm handles different voices, microphones, and acoustic environments.
    \item \textbf{Computational Efficiency}: The computational resources required, affecting the feasibility for real-time applications.
    \item \textbf{Implementation Complexity}: The effort required to implement and integrate the algorithm.
\end{itemize}

\subsection{Implementation}
We implemented pitch detection using both the aubio and librosa libraries in Python. The following code snippet shows our implementation of the YIN algorithm using aubio:

\begin{lstlisting}[language=Python]
def detect_pitch_aubio(file_path, method="yin", 
                      buffer_size=2048, hop_size=512, 
                      sample_rate=44100):
    # Create pitch object
    pitch_o = aubio.pitch(method, buffer_size, hop_size, 
                         sample_rate)
    pitch_o.set_unit("Hz")
    pitch_o.set_silence(-40)
    pitch_o.set_tolerance(0.8)
    
    # Load audio file
    source = aubio.source(file_path, sample_rate, hop_size)
    sample_rate = source.samplerate
    
    # Lists to store results
    pitches = []
    confidences = []
    
    # Process audio file
    while True:
        samples, read = source()
        pitch = pitch_o(samples)[0]
        confidence = pitch_o.get_confidence()
        
        pitches.append(float(pitch))
        confidences.append(float(confidence))
        
        if read < hop_size:
            break
    
    # Convert frame indices to time
    times = [t * hop_size / float(sample_rate) 
            for t in range(len(pitches))]
    
    return times, pitches, confidences
\end{lstlisting}

For pYIN, we used the implementation provided by librosa:

\begin{lstlisting}[language=Python]
def detect_vocal_pitch(file_path, hop_length=512, 
                      fmin=80.0, fmax=800.0):
    # Load audio file
    y, sr = librosa.load(file_path, sr=None)
    
    # Use pYIN algorithm
    f0, voiced_flag, voiced_probs = librosa.pyin(
        y, 
        fmin=fmin,
        fmax=fmax,
        sr=sr,
        hop_length=hop_length,
        fill_na=None  # Don't fill unvoiced sections
    )
    
    # Convert frame indices to time
    times = librosa.times_like(f0, sr=sr, 
                             hop_length=hop_length)
    
    return times, f0, voiced_probs
\end{lstlisting}

\subsection{Post-Processing Techniques}
To improve the quality of pitch tracking specifically for vocal applications, we implemented several post-processing techniques:

\subsubsection{Energy Thresholding}
We used the root mean square (RMS) energy of the signal to distinguish between voiced and unvoiced segments:

\begin{lstlisting}[language=Python]
# Calculate energy for voice activity detection
energy = librosa.feature.rms(
    y=y, frame_length=hop_length*2, 
    hop_length=hop_length)[0]
energy = energy / np.max(energy) if np.max(energy) > 0 else energy

# Apply threshold to remove low-confidence segments
for i in range(len(f0)):
    if confidence[i] > energy_threshold and f0[i] > 0:
        processed_pitch[i] = f0[i]
    else:
        processed_pitch[i] = 0
\end{lstlisting}

\subsubsection{Continuity Constraints}
To avoid octave jumps and other pitch tracking errors, we implemented continuity constraints that penalize large pitch changes between consecutive frames:

\begin{lstlisting}[language=Python]
# Apply continuity constraints to avoid octave jumps
for i in range(1, len(processed_pitch)):
    if processed_pitch[i] > 0 and processed_pitch[i-1] > 0:
        # Calculate octave difference
        octave_diff = np.abs(np.log2(
            processed_pitch[i] / processed_pitch[i-1]))
        
        # If jump is too large, try to correct it
        if octave_diff > continuity_tolerance:
            # Check if it's likely an octave error
            if abs(octave_diff - 1.0) < 0.1:  # Close to an octave jump
                # Adjust to previous octave if confidence allows
                if confidence[i] < confidence[i-1] * (1 + octave_cost):
                    if processed_pitch[i] > processed_pitch[i-1]:
                        processed_pitch[i] = processed_pitch[i] / 2.0
                    else:
                        processed_pitch[i] = processed_pitch[i] * 2.0
\end{lstlisting}

\subsubsection{Median Filtering}
To smooth the pitch contour and reduce jitter, we applied median filtering to segments of detected pitch:

\begin{lstlisting}[language=Python]
# Apply median filtering to smooth the pitch contour
valid_indices = processed_pitch > 0
if np.any(valid_indices):
    # Create a copy for filtering
    smoothed_pitch = np.copy(processed_pitch)
    
    # Only apply filtering to segments with valid pitch
    segments = []
    segment_start = None
    
    # Find continuous segments
    for i in range(len(valid_indices)):
        if valid_indices[i] and segment_start is None:
            segment_start = i
        elif not valid_indices[i] and segment_start is not None:
            segments.append((segment_start, i))
            segment_start = None
    
    # Add the last segment if it exists
    if segment_start is not None:
        segments.append((segment_start, len(valid_indices)))
    
    # Apply median filtering to each segment
    for start, end in segments:
        if end - start > median_filter_size:
            segment = processed_pitch[start:end]
            smoothed_segment = medfilt(segment, median_filter_size)
            smoothed_pitch[start:end] = smoothed_segment
    
    processed_pitch = smoothed_pitch
\end{lstlisting}

\subsection{Visualization System}
We developed a piano roll visualization system using PyQt6 to provide intuitive feedback to users. The visualization includes:

\begin{itemize}
    \item A piano roll display with evenly spaced pitch lines
    \item Colored key indicators showing black and white piano keys
    \item Connected pitch lines showing the continuous pitch trajectory
    \item Key highlighting to indicate the currently sung note
\end{itemize}

The visualization maps detected pitch to musical notes using the following frequency-to-MIDI conversion:

\begin{equation}
MIDI = 69 + 12 \times \log_2\left(\frac{f}{440}\right)
\end{equation}

where $f$ is the frequency in Hz, and 69 corresponds to A4 (440 Hz).

\section{Results and Discussion}

\subsection{Algorithm Comparison}
We evaluated the performance of different pitch detection algorithms on vocal recordings. Table \ref{tab:algorithm_comparison} summarizes our findings.

\begin{table}[htbp]
\caption{Comparison of Pitch Detection Algorithms for Vocal Applications}
\begin{center}
\begin{tabular}{|l|c|c|c|c|}
\hline
\textbf{Algorithm} & \textbf{Accuracy} & \textbf{Latency} & \textbf{CPU Usage} & \textbf{Robustness} \\
\hline
YIN & Medium & Low & Low & Medium \\
\hline
pYIN & High & Medium & Medium & High \\
\hline
CREPE & Very High & High & Very High & Very High \\
\hline
MPM & Medium & Low & Low & Medium \\
\hline
\end{tabular}
\label{tab:algorithm_comparison}
\end{center}
\end{table}

Our experiments showed that pYIN consistently outperformed the basic YIN algorithm for vocal pitch tracking, particularly in handling vibrato and note transitions. CREPE achieved the highest accuracy but at the cost of significantly higher computational requirements, making it less suitable for real-time applications on devices with limited resources.

\subsection{Post-Processing Effectiveness}
The post-processing techniques we implemented significantly improved the quality of pitch tracking. Table \ref{tab:post_processing} shows the impact of different techniques.

\begin{table}[htbp]
\caption{Impact of Post-Processing Techniques on Pitch Tracking Quality}
\begin{center}
\begin{tabular}{|l|c|c|c|}
\hline
\textbf{Technique} & \textbf{Improvement} & \textbf{CPU Cost} & \textbf{Latency Impact} \\
\hline
Energy Thresholding & High & Low & Negligible \\
\hline
Continuity Constraints & Medium & Low & Negligible \\
\hline
Median Filtering & High & Low & Medium \\
\hline
All Combined & Very High & Medium & Medium \\
\hline
\end{tabular}
\label{tab:post_processing}
\end{center}
\end{table}

Energy thresholding was particularly effective at removing spurious pitch detections during unvoiced segments, while median filtering significantly reduced jitter in the pitch contour. The combination of all techniques provided the best results, with only a moderate increase in computational cost.

\subsection{Visualization Effectiveness}
User testing of the piano roll visualization indicated that it provided intuitive feedback on pitch accuracy. The evenly spaced pitch lines were found to be more readable than a traditional piano keyboard visualization, while still maintaining the visual connection to musical notes through the colored key indicators.

\section{Vocal Line Extraction}

In addition to pitch detection, our karaoke training application requires the ability to isolate vocal lines from mixed audio recordings. This capability allows users to practice with just the vocal line, helping them learn melodies more effectively before attempting to sing along with the full accompaniment.

\subsection{Motivation}
Vocal line extraction serves several important purposes in a karaoke training application:
\begin{itemize}
    \item Enables users to hear the target melody in isolation
    \item Provides a reference for pitch and timing without instrumental distractions
    \item Allows for direct comparison between the user's voice and the original vocal
    \item Facilitates progressive learning approaches (vocal only → vocal with accompaniment → accompaniment only)
\end{itemize}

\subsection{Algorithms Evaluated}
We evaluated three approaches to vocal line extraction:

\subsubsection{Traditional DSP Methods (Librosa)}
While Librosa provides excellent audio processing capabilities, its harmonic-percussive source separation (HPSS) \cite{fitzgerald2010hpss} is not specifically designed for vocal isolation:

\begin{lstlisting}[language=Python]
import librosa
import numpy as np

# Load the audio file
y, sr = librosa.load('song.mp3', sr=None)

# Perform harmonic-percussive source separation
y_harmonic, y_percussive = librosa.effects.hpss(y)

# The harmonic component contains vocals but also other harmonic instruments
\end{lstlisting}

This approach proved insufficient for clean vocal isolation, as it doesn't specifically target vocals and produces significant bleed-through from other harmonic instruments.

\subsubsection{Spleeter (Deezer)}
Spleeter \cite{stoter2020spleeter} is an open-source music source separation library developed by Deezer Research that uses deep learning to separate audio into different stems:

\begin{lstlisting}[language=Python]
from spleeter.separator import Separator
import numpy as np

# Initialize the separator with the desired model
separator = Separator('spleeter:4stems')

# Load and separate audio
waveform, sr = librosa.load('song.mp3', sr=44100, mono=False)
# Ensure correct shape (samples, channels)
if waveform.shape[0] == 2:
    waveform = waveform.T

# Apply high-pass filter to improve vocal clarity
from scipy import signal
cutoff = 150  # Hz
nyquist = 0.5 * sr
normal_cutoff = cutoff / nyquist
b, a = signal.butter(4, normal_cutoff, btype='high', analog=False)
filtered = np.zeros_like(waveform)
for i in range(waveform.shape[1]):
    filtered[:, i] = signal.filtfilt(b, a, waveform[:, i])

# Separate the audio
prediction = separator.separate(filtered)
vocals = prediction['vocals']
\end{lstlisting}

We tested Spleeter with multiple model configurations:
\begin{itemize}
    \item 2 stems (vocals + accompaniment)
    \item 4 stems (vocals, drums, bass, other)
    \item 5 stems (vocals, drums, bass, piano, other)
\end{itemize}

\subsubsection{Demucs (Facebook Research)}
Demucs (Deep Extractor for Music Sources) \cite{defossez2019demucs} is a state-of-the-art music source separation model developed by Facebook Research:

\begin{lstlisting}[language=Python]
import torch
from demucs.pretrained import get_model
from demucs.apply import apply_model
import numpy as np
import librosa

# Load the model
model = get_model('htdemucs')
model.cpu()
model.eval()

# Load and process audio
wav, sr = librosa.load('song.mp3', sr=model.samplerate, mono=False)
if wav.ndim == 1:
    wav = np.stack([wav, wav])

# Apply the model
wav = torch.tensor(wav)
with torch.no_grad():
    sources = apply_model(model, wav[None])[0]

# Extract vocals
sources = sources.cpu().numpy()
vocals = sources[model.sources.index('vocals')]
\end{lstlisting}

\subsection{Evaluation Methodology}
We evaluated each algorithm based on:
\begin{itemize}
    \item \textbf{Vocal Isolation Quality}: How cleanly the vocals were separated from instruments
    \item \textbf{Processing Speed}: Time required to process a 3:09 minute song
    \item \textbf{Implementation Complexity}: Ease of integration into our application
    \item \textbf{Architecture Compatibility}: Performance on Apple Silicon (arm64) hardware
\end{itemize}

Testing was performed on an Apple Silicon Mac using a Python 3.11 virtual environment with arm64 architecture. We used the song "Code Monkey" by Jonathan Coulton (3:09 in length) as our primary test case.

\subsection{Results}

\subsubsection{Vocal Isolation Quality}
Our extensive listening tests revealed significant differences in vocal isolation quality across the evaluated algorithms. Demucs consistently demonstrated superior vocal isolation with minimal instrumental bleed-through, producing the cleanest separation between vocals and accompaniment. The Spleeter 5-stem model performed reasonably well but exhibited noticeable instrumental artifacts in the vocal track. The Spleeter 4-stem model provided moderate separation quality, while the 2-stem model showed significant instrumental bleed-through. As expected, Librosa's HPSS, which was not specifically designed for vocal isolation, performed poorly in this task. Table \ref{tab:vocal_isolation} summarizes these findings.

\begin{table}[htbp]
\caption{Comparison of Vocal Isolation Quality}
\begin{center}
\begin{tabular}{|l|c|c|c|}
\hline
\textbf{Algorithm} & \textbf{Isolation Quality} & \textbf{Artifacts} & \textbf{Bleed-through} \\
\hline
Demucs & Excellent & Minimal & Very Low \\
\hline
Spleeter (5-stem) & Good & Moderate & Low \\
\hline
Spleeter (4-stem) & Moderate & Moderate & Medium \\
\hline
Spleeter (2-stem) & Fair & High & High \\
\hline
Librosa HPSS & Poor & Very High & Very High \\
\hline
\end{tabular}
\label{tab:vocal_isolation}
\end{center}
\end{table}

\subsubsection{Processing Speed and Implementation Challenges}
The processing times for a 3:09 minute song revealed an inverse relationship between quality and speed. Librosa's HPSS was the fastest at approximately 3 seconds, followed by Spleeter's 2-stem model at 8.88 seconds, 4-stem model at 11.32 seconds, and 5-stem model at 12.99 seconds. Demucs was significantly slower, requiring exactly 56.66 seconds to process the same audio file. These performance differences reflect the complexity of the underlying models and their computational requirements. Table \ref{tab:processing_speed} provides a comparison of processing times and implementation considerations.

\begin{table}[htbp]
\caption{Processing Speed and Implementation Considerations}
\begin{center}
\begin{tabular}{|l|c|c|c|}
\hline
\textbf{Algorithm} & \textbf{Processing Time} & \textbf{Implementation} & \textbf{Architecture} \\
\textbf{} & \textbf{(3:09 song)} & \textbf{Complexity} & \textbf{Compatibility} \\
\hline
Librosa HPSS & $\sim$3.00 seconds & Low & High \\
\hline
Spleeter (2-stem) & 8.88 seconds & Medium & Medium \\
\hline
Spleeter (4-stem) & 11.32 seconds & Medium & Medium \\
\hline
Spleeter (5-stem) & 12.99 seconds & Medium & Medium \\
\hline
Demucs & 56.66 seconds & Medium & Medium \\
\hline
\end{tabular}
\label{tab:processing_speed}
\end{center}
\end{table}

Both Spleeter and Demucs presented implementation challenges on Apple Silicon architecture. Spleeter required specific versions of TensorFlow (2.12.0) and NumPy (1.23.5) to work properly, and it uses deprecated TensorFlow APIs (Estimator) that may cause future compatibility issues. Demucs implementation was more straightforward but required PyTorch with MPS acceleration for reasonable performance on Apple Silicon. Both algorithms required careful handling of audio format (samples vs. channels orientation) and sample rate consistency to function correctly.

\subsubsection{Optimizations}
For Spleeter, we implemented several optimizations based on best practices:
\begin{itemize}
    \item Applied high-pass filtering (150 Hz) to improve vocal clarity
    \item Ensured proper audio format handling (samples, channels)
    \item Used local model caching to avoid repeated downloads
\end{itemize}

\subsection{Discussion}
Our evaluation revealed a clear trade-off between vocal isolation quality and processing speed. Demucs consistently produced superior vocal isolation but at the cost of significantly longer processing times. Spleeter offered faster processing but with more instrumental bleed-through, even when using the more complex 5-stem model.

For our karaoke training application, where accurate pitch tracking depends on clean vocal isolation, we determined that the superior quality of Demucs outweighs the performance penalty. The cleaner vocal isolation enables more accurate pitch detection and provides a better reference for users learning to sing a melody.

\section{Conclusion and Future Work}
Our research and development of the PitchTrack application has demonstrated that pYIN, combined with appropriate post-processing techniques, offers the best balance of accuracy and computational efficiency for vocal pitch tracking in a karaoke training application. The piano roll visualization system provides intuitive feedback to users, helping them improve their singing pitch accuracy.

For vocal line extraction, Demucs emerged as the superior solution despite its higher computational cost. The clean vocal isolation it provides is essential for accurate pitch detection and for providing users with high-quality reference audio for practice. While Spleeter offers faster processing, the quality difference justifies the performance trade-off for our application's needs.

Future work could explore:
\begin{itemize}
    \item Real-time microphone input for live singing practice
    \item Integration of CREPE for offline analysis with higher accuracy
    \item Adaptive thresholding based on voice characteristics
    \item Performance metrics and scoring for user feedback
    \item Multi-voice pitch tracking for harmony training
    \item Optimizations to improve Demucs processing speed
    \item Hybrid approaches combining the speed of Spleeter with the quality of Demucs
    \item Progressive learning modes that gradually reduce the volume of the reference vocal
\end{itemize}

\section*{AI Disclosure}
This research and paper were developed with the assistance of Amazon Q Developer CLI with Anthropic Claude Sonnet 3.7, an AI assistant. The AI was used both in conducting the research, analyzing code, and in preparing this article. The human author (David Rostcheck) provided direction, domain expertise, and final editorial oversight.

\begin{thebibliography}{00}
\bibitem{cheveigne2002yin} A. de Cheveigné and H. Kawahara, "YIN, a fundamental frequency estimator for speech and music," The Journal of the Acoustical Society of America, vol. 111, no. 4, pp. 1917-1930, 2002.

\bibitem{mauch2014pyin} M. Mauch and S. Dixon, "pYIN: A fundamental frequency estimator using probabilistic threshold distributions," in Proc. IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP), pp. 659-663, 2014.

\bibitem{kim2018crepe} J. W. Kim, J. Salamon, P. Li, and J. P. Bello, "CREPE: A convolutional representation for pitch estimation," in Proc. IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP), pp. 161-165, 2018.

\bibitem{mcleod2005fast} P. McLeod and G. Wyvill, "A smarter way to find pitch," in Proc. International Computer Music Conference (ICMC), 2005.

\bibitem{gerhard2003pitch} D. Gerhard, "Pitch extraction and fundamental frequency: History and current techniques," Technical Report TR-CS 2003-06, Department of Computer Science, University of Regina, 2003.

\bibitem{fitzgerald2010hpss} D. Fitzgerald, "Harmonic/percussive separation using median filtering," in Proc. Int. Conf. Digital Audio Effects (DAFx), Graz, Austria, Sep. 2010, pp. 246–253.

\bibitem{stoter2020spleeter} F. Stöter, S. Uhlich, A. Liutkus, and Y. Mitsufuji, "Spleeter: A Fast and State-of-the-Art Music Source Separation Tool with Pre-trained Models," Journal of Open Source Software, vol. 5, no. 50, pp. 2154, 2020.

\bibitem{defossez2019demucs} A. Defossez, S. Watanabe, E. Vincent, and N. Usunier, "Demucs: Deep Extractor for Music Sources with Improved Source Separation," arXiv preprint arXiv:1909.01174, 2019.

\end{thebibliography}

\end{document}
